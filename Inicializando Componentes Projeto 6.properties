##### Inicializando o ZooKeeper #####

1- Abra o terminal, navegue até a pasta do Kafka e execute o comando abaixo para ((((inicializar o Zookeepper)))) (gerenciador de cluster do Kafka).

cd /opt/kafka

bin/zookeeper-server-start.sh config/zookeeper.properties


##### Inicializando o Kafka ########


1-  Abra outro terminal, navegue até a pasta do Kafka e execute o comando abaixo para ((((inicializar o Kafka))))).

cd /opt/kafka

bin/kafka-server-start.sh config/server.properties


2 - Abra outro terminal, navegue até a pasta do Kafka e execute o comando abaixo para ((((criar um tópico)))) no Kafka.
Este procedimento só precisa ser feiti uma única vez:


bin/kafka-topics.sh --create --topic logs  --zookeeper localhost:2181


3 - No mesmo terminal anterior execute o comando abaixo para (((descrever o tópico)))).


bin/kafka-topics.sh --describe --topic logs --zookeeper localhost:2181


#### Inicialização do Flume ####

- Inicie o agente Flume usando o arquivo de configuração flume.conf, no diretório oficial do Flume, para coemçar a gerar dados:

cd $FLUME_HOME


bin/flume-ng agent --conf conf/ -f conf/flume.conf -Dflume.root.logger=INFO,console -n agent


#### Iniciando o kafka Producer #####

1 -  No mesmo terminal anterior, do kafka,  execute o comando abaixo para ((((produzir o streaming)))) de dados no Kafka (como um produtor de streaming).


bin/kafka-console-producer.sh --broker-list localhost:9092 --topic logs


2 - No mesmo terminal anterior execute o comando abaixo para ´(((((listar o conteúdo do tópico (como um consumidor de streaming))))))).

bin/kafka-console-consumer.sh --topic logs --from-beginning --bootstrap-server localhost:9092


#### Inicializando o SparkStreaming ######

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 LogProcessing.py 



#### Inicializando o HBase ####

start-hbase.sh

# Verificando arquivos de log:

cd /opt/hbase/log

# status do nó master

hbase hbck 


##### Criação de uma tabela no Hbase (EXEMPLO) #######

# Abrir o shell

	hbase shell

# Limpando arquivos de log no diretório /opt/hbase/logs:

rm -rf *


# Cria uma tabela( Clientes) com uma família de colunas (column family) para receber os dados

	create 'nasa', 'dados_nasa'

 
Exemplo: 

create_namespace 'log_data'

# Este comando cria um novo namespace chamado "log_data".

create 'log_data:log_table', 'data:timestamp', 'data:ip', 'data:url', 'data:status', 'data:bytes', 'data:referer', 'data:user_agent', 'data:request_method'

# Este comando cria uma nova tabela chamada "log_table" no namespace "log_data", com as colunas
#  "timestamp", "ip", "url", "status", "bytes", "referer", "user_agent" e "request_method".

# Você pode modificar as colunas e famílias de acordo com as suas necessidades.


